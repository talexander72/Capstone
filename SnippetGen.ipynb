{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "log\n",
      "[3, 0, 1, 2, 3]\n",
      "['SpectralFlatness', 'SpectralCentroid', 'SpectralCrestFactor', 'SpectralDecrease', 'SpectralFlatness']\n"
     ]
    }
   ],
   "source": [
    "logaccuracyspread = [.78,.80,.87,.90,.95,.96,.97,.97,.97,.98,.97,.97,.95,.95,.95,.94,.93]\n",
    "svmaccuracyspread = [.74,.80,.83,.88,.92,.93,.94,.95,.96,.96,.96,.95,.95,.94,.95,.93,.91]\n",
    "knnaccuracyspread = [.75,.80,.85,.88,.94,.96,.97,.97,.96,.96,.96,.96,.95,.95,.95,.95,.95]\n",
    "rfaccuracyspread = [.69,.79,.90,.91,.94,.96,.97,.98,.97,.97,.97,.97,.97,.97,.96,.96,.96]\n",
    "\n",
    "sfsInds = sfs.indices_\n",
    "\n",
    "feature_list = ['SpectralCentroid', 'SpectralCrestFactor', 'SpectralDecrease', 'SpectralFlatness', 'SpectralFlux',\n",
    "                    'SpectralRolloff', 'SpectralSkewness', 'SpectralSpread', 'SpectralTonalPowerRatio']\n",
    "\n",
    "performancePreference = \"tradeoff\" #highest, fewest, tradeoff\n",
    "\n",
    "if performancePreference == \"highest\":\n",
    "    maxVals = np.array([logaccuracyspread[np.argmax(logaccuracyspread)],\n",
    "    svmaccuracyspread[np.argmax(svmaccuracyspread)],\n",
    "    knnaccuracyspread[np.argmax(knnaccuracyspread)],\n",
    "    rfaccuracyspread[np.argmax(rfaccuracyspread)]])\n",
    "\n",
    "    numFeats = np.array([np.argmax(logaccuracyspread),\n",
    "    np.argmax(svmaccuracyspread),\n",
    "    np.argmax(knnaccuracyspread),\n",
    "    np.argmax(rfaccuracyspread)])\n",
    "\n",
    "    maxInds = np.where(maxVals == max(maxVals))\n",
    "    featInd = [] \n",
    "    for i in maxInds[0]:\n",
    "        featInd.append(numFeats[i])\n",
    "\n",
    "    leastFeats = np.where(featInd == min(featInd))\n",
    "\n",
    "    optimalFeatNum = featInd[leastFeats[0][0]] + 1\n",
    "\n",
    "    if maxInds[0][leastFeats[0][0]] == 0:\n",
    "        optimalModel = \"log\"\n",
    "    elif maxInds[0][leastFeats[0][0]] == 1:\n",
    "        optimalModel = \"svm\"\n",
    "    elif maxInds[0][leastFeats[0][0]] == 2:\n",
    "        optimalModel = \"knn\"\n",
    "    elif maxInds[0][leastFeats[0][0]] == 3:\n",
    "        optimalModel = \"rfa\"\n",
    "elif performancePreference == \"fewest\":\n",
    "    lowestVals = [np.argmax(np.array(logaccuracyspread) > .8),\n",
    "    np.argmax(np.array(svmaccuracyspread) > .8),\n",
    "    np.argmax(np.array(knnaccuracyspread) > .8),\n",
    "    np.argmax(np.array(rfaccuracyspread) > .8)]\n",
    "\n",
    "    minInd = np.where(lowestVals == min(lowestVals))\n",
    "\n",
    "    optimalFeatNum = lowestVals[minInd[0][0]] + 1\n",
    "\n",
    "    if minInd[0][0] == 0:\n",
    "        optimalModel = \"log\"\n",
    "    elif minInd[0][0] == 1:\n",
    "        optimalModel = \"svm\"\n",
    "    elif minInd[0][0] == 2:\n",
    "        optimalModel = \"knn\"\n",
    "    elif minInd[0][0] == 3:\n",
    "        optimalModel = \"rfa\"\n",
    "elif performancePreference == \"tradeoff\":\n",
    "    for i in range(0,len(logaccuracyspread)-3):\n",
    "        if logaccuracyspread[i+1] - logaccuracyspread[i] < 0.05 and logaccuracyspread[i+2] - logaccuracyspread[i] < 0.05 and logaccuracyspread[i+3] - logaccuracyspread[i] < 0.05:\n",
    "            logInd = i\n",
    "            break\n",
    "    for i in range(0,len(svmaccuracyspread)-3):\n",
    "        if svmaccuracyspread[i+1] - svmaccuracyspread[i] < 0.05 and svmaccuracyspread[i+2] - svmaccuracyspread[i] < 0.05 and svmaccuracyspread[i+3] - svmaccuracyspread[i] < 0.05:\n",
    "            svmInd = i\n",
    "            break\n",
    "    for i in range(0,len(knnaccuracyspread)-3):\n",
    "        if knnaccuracyspread[i+1] - knnaccuracyspread[i] < 0.05 and knnaccuracyspread[i+2] - knnaccuracyspread[i] < 0.05 and knnaccuracyspread[i+3] - knnaccuracyspread[i] < 0.05:\n",
    "            knnInd = i\n",
    "            break\n",
    "    for i in range(0,len(rfaccuracyspread)-3):\n",
    "        if rfaccuracyspread[i+1] - rfaccuracyspread[i] < 0.05 and rfaccuracyspread[i+2] - rfaccuracyspread[i] < 0.05 and rfaccuracyspread[i+3] - rfaccuracyspread[i] < 0.05:\n",
    "            rfaInd = i\n",
    "            break\n",
    "        \n",
    "    inds = [logInd, svmInd, knnInd, rfaInd]\n",
    "    minInd = np.where(np.array(inds) == min(inds))\n",
    "\n",
    "    optimalFeatNum = inds[minInd[0][0]] + 1\n",
    "\n",
    "    if minInd[0][0] == 0:\n",
    "        optimalModel = \"log\"\n",
    "    elif minInd[0][0] == 1:\n",
    "        optimalModel = \"svm\"\n",
    "    elif minInd[0][0] == 2:\n",
    "        optimalModel = \"knn\"\n",
    "    elif minInd[0][0] == 3:\n",
    "        optimalModel = \"rfa\"\n",
    "\n",
    "\n",
    "print(optimalFeatNum)\n",
    "print(optimalModel) #model to be used for snippet\n",
    "\n",
    "\n",
    "optimalFeatInds = sfsInds[0:optimalFeatNum]\n",
    "print(optimalFeatInds)\n",
    "\n",
    "featList = [] #features to be used for snippet\n",
    "for i in optimalFeatInds:\n",
    "    featList.append(feature_list[i])\n",
    "    \n",
    "print(featList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 4, 3]\n",
      "(array([3], dtype=int64),)\n",
      "3\n",
      "rfa\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snippetGen(path, features, model, chunk, hop):\n",
    "    \n",
    "    feature_list = features\n",
    "    feature_names = features\n",
    "    \n",
    "    snipImports = [\n",
    "        'from scipy.io.wavfile import read',\n",
    "        'import os',\n",
    "        'import pandas as pd',\n",
    "        'import numpy as np',\n",
    "        'import math',\n",
    "        'import matplotlib.pyplot as plt',\n",
    "        'from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg',\n",
    "        'import pyACA',\n",
    "        'from sklearn.model_selection import StratifiedKFold, KFold',\n",
    "        'from sklearn.model_selection import train_test_split',\n",
    "        'from sklearn.linear_model import LogisticRegression',\n",
    "        'from sklearn import svm',\n",
    "        'from sklearn.neighbors import KNeighborsClassifier',\n",
    "        'from sklearn.ensemble import RandomForestClassifier',\n",
    "        'from sklearn.preprocessing import StandardScaler',\n",
    "        'from sklearn.pipeline import make_pipeline',\n",
    "        'from sklearn.metrics import accuracy_score',\n",
    "        'from itertools import combinations',\n",
    "        'from sklearn.base import clone',\n",
    "        'from sklearn.metrics import accuracy_score',\n",
    "        'from sklearn.metrics import balanced_accuracy_score',\n",
    "        'from sklearn.metrics import confusion_matrix'\n",
    "    ]\n",
    "    \n",
    "    snipReadData = [\n",
    "        'def readData(path, chunk, hop):',\n",
    "        '    #chunk = values[\\'-CHUNK-\\']',\n",
    "        '    #hop = (1 - (values[\\'-HOP-\\']) / 100) * chunk',\n",
    "        '    class_names = os.listdir(path)',\n",
    "        '    filenames_1 = os.listdir(path + \\'/\\' + class_names[1])',\n",
    "        '    filenames_2 = os.listdir(path + \\'/\\' + class_names[2])',\n",
    "        '    num_files = len(filenames_1) + len(filenames_2)',\n",
    "        '    time_series_a = []  # initializing variables',\n",
    "        '    time_series_b = []',\n",
    "        '    bool_check = True',\n",
    "        '    for i in range(num_files):  # step 1 progress bar',\n",
    "        '        if not sg.one_line_progress_meter(\\'File Reading Progress\\', i + 1, num_files, \\'step 1 of 3: file reading\\'):',\n",
    "        '            break',\n",
    "        '        if i < len(filenames_1):    # processing class A',\n",
    "        '            [current_fs1, x] = read(path + \\'/\\' + class_names[1] + \\'/\\' + filenames_1[i])',\n",
    "        '            x = x[:, 0]  # grabbing only channel 1 of recording',\n",
    "        '            time_series_a.extend(x)',\n",
    "        '            if bool_check:  # grabbing the first returned sampling-rate to check subsequent values against',\n",
    "        '                sampling_rate_check = current_fs1',\n",
    "        '                bool_check = False',\n",
    "        '            else:',\n",
    "        '                if sampling_rate_check != current_fs1:',\n",
    "        '                    raise Warning(\\'Please make sure all files have the same sampling rate\\')',\n",
    "        '        else:   # processing class B',\n",
    "        '            [current_fs2, x2] = read(values[\\'-PATH-\\'] + \\'/\\' + class_names[2] + \\'/\\' + filenames_2[i - len(filenames_1)])',\n",
    "        '            x2 = x2[:, 0]  # grabbing only channel 1',\n",
    "        '            time_series_b.extend(x2)',\n",
    "        '            if sampling_rate_check != current_fs2:',\n",
    "        '                raise Warning(\\'Please make sure all files have the same sampling rate\\')',\n",
    "        '    return chunk, hop, time_series_a, time_series_b, sampling_rate_check'\n",
    "    ]\n",
    "    \n",
    "    snipGetFeatures = [\n",
    "        'def getFeatures(time_series_a, time_series_b, chunk, hop, sampling_rate):',\n",
    "        '    def helpGetFeatures(feature, file, f_s):',\n",
    "        '        [vsf, t] = pyACA.computeFeature(feature, file, f_s, iBlockLength=chunk, iHopLength=hop)',\n",
    "        '        return vsf',\n",
    "        '    time_series_a = np.array(time_series_a)     # initializing variables',\n",
    "        '    time_series_b = np.array(time_series_b)',\n",
    "        '    features_a = []',\n",
    "        '    features_b = []',\n",
    "        '    for f in range(len(feature_names)):',\n",
    "        '        if not sg.one_line_progress_meter(\\'Feature Extraction Progress\\', f + 1, len(feature_names),',\n",
    "        '                                          \\'step 2 of 3: feature extraction\\'):',\n",
    "        '            break',\n",
    "        '        current_feature_a = helpGetFeatures(feature_list[f], time_series_a, sampling_rate)',\n",
    "        '        features_a.append(current_feature_a)',\n",
    "        '        current_feature_b = helpGetFeatures(feature_list[f], time_series_b, sampling_rate)',\n",
    "        '        features_b.append(current_feature_b)',\n",
    "        '    return features_a, features_b'\n",
    "    ]\n",
    "    \n",
    "    snipFormatData = [\n",
    "        'def formatData():',\n",
    "        '    data1 = np.ones(len(features_list_a[0]))',\n",
    "        '    data2 = np.zeros(len(features_list_b[0]))',\n",
    "        '    data3 = np.concatenate((data1, data2))',\n",
    "        '    data5 = []',\n",
    "        '    for d in range(len(feature_names)):',\n",
    "        '        data4 = np.concatenate([features_list_a[d], features_list_b[d]])',\n",
    "        '        data5.append(data4)',\n",
    "        '    categories = pd.DataFrame(data3)',\n",
    "        '    predictors = pd.DataFrame(data5).transpose()',\n",
    "        '    predictors.columns = feature_names',\n",
    "        '    predictors_scaled = predictors.copy()  # normalization of audio features',\n",
    "        '    for i in feature_names:',\n",
    "        '        predictors_scaled[i] = (predictors_scaled[i] - predictors_scaled[i].min())'+ str(' / \\\\'),\n",
    "        '                               (predictors_scaled[i].max() - predictors_scaled[i].min())',\n",
    "        '    training_categories, testing_categories, training_predictors, testing_predictors \\\\',\n",
    "        '        = train_test_split(categories, predictors_scaled, test_size=.2, random_state=25)',\n",
    "        '    return training_categories, testing_categories, training_predictors, testing_predictors'\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    snipInitializeModels = [\n",
    "        'def initializeModels():',\n",
    "        '    model1 = LogisticRegression(solver=\\'lbfgs\\', max_iter=200)',\n",
    "        '    model2 = svm.SVC()',\n",
    "        '    model3 = KNeighborsClassifier(n_neighbors=3)',\n",
    "        '    model4 = RandomForestClassifier(max_depth=2, random_state=0)',\n",
    "        '    return model1, model2, model3, model4',\n",
    "        '',\n",
    "        '[model1, model2, model3, model4] = initializeModels()'\n",
    "    ]\n",
    "    \n",
    "    if model == 'log':\n",
    "        snipOptimalModel = 'optimalModel = model1'\n",
    "    elif model == 'svm':\n",
    "        snipOptimalModel = 'optimalModel = model2'\n",
    "    elif model == 'knn':\n",
    "        snipOptimalModel = 'optimalModel = model3'\n",
    "    elif model == 'rfa':\n",
    "        snipOptimalModel = 'optimalModel = model4'\n",
    "    \n",
    "    snipExecution = [\n",
    "        '[chunk, hop, time_series_a, time_series_b, sampling_rate_check] = readData(\\''+str(path)+'\\', '+str(chunk)+', '+str(hop)+')',\n",
    "        '[features_a, features_b] = getFeatures(time_series_a, time_series_b, chunk, hop, sampling_rate_check)',\n",
    "        '[training_categories, testing_categories, training_predictors, testing_predictors] = formatData(features_a, features_b)',\n",
    "        'optimalModel.fit(training_predictors, training_categories)',\n",
    "        'predictions = optimalModel.predict(testing_predictors)',\n",
    "        'cm = confusion_matrix(testing_categories, predictions)',\n",
    "        'pd.DataFrame(cm)',\n",
    "        'accuracy = (cm[0,0]+cm[1,1]) / sum(sum(cm))'\n",
    "    ]\n",
    "    \n",
    "    print(*snipImports, sep = '\\n')\n",
    "    print('\\n')\n",
    "    print(*snipReadData, sep = '\\n')\n",
    "    print('\\n')\n",
    "    print(*snipGetFeatures, sep = '\\n')\n",
    "    print('\\n')\n",
    "    print(*snipFormatData, sep = '\\n')\n",
    "    print('\\n')\n",
    "    print(*snipInitializeModels, sep = '\\n') \n",
    "    print('\\n')\n",
    "    print(snipOptimalModel)\n",
    "    print('\\n')\n",
    "    print(*snipExecution, sep = '\\n')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from scipy.io.wavfile import read\n",
      "import os\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import math\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
      "import pyACA\n",
      "from sklearn.model_selection import StratifiedKFold, KFold\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import svm\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.pipeline import make_pipeline\n",
      "from sklearn.metrics import accuracy_score\n",
      "from itertools import combinations\n",
      "from sklearn.base import clone\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import balanced_accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "\n",
      "def readData(path, chunk, hop):\n",
      "    #chunk = values['-CHUNK-']\n",
      "    #hop = (1 - (values['-HOP-']) / 100) * chunk\n",
      "    class_names = os.listdir(path)\n",
      "    filenames_1 = os.listdir(path + '/' + class_names[1])\n",
      "    filenames_2 = os.listdir(path + '/' + class_names[2])\n",
      "    num_files = len(filenames_1) + len(filenames_2)\n",
      "    time_series_a = []  # initializing variables\n",
      "    time_series_b = []\n",
      "    bool_check = True\n",
      "    for i in range(num_files):  # step 1 progress bar\n",
      "        if not sg.one_line_progress_meter('File Reading Progress', i + 1, num_files, 'step 1 of 3: file reading'):\n",
      "            break\n",
      "        if i < len(filenames_1):    # processing class A\n",
      "            [current_fs1, x] = read(path + '/' + class_names[1] + '/' + filenames_1[i])\n",
      "            x = x[:, 0]  # grabbing only channel 1 of recording\n",
      "            time_series_a.extend(x)\n",
      "            if bool_check:  # grabbing the first returned sampling-rate to check subsequent values against\n",
      "                sampling_rate_check = current_fs1\n",
      "                bool_check = False\n",
      "            else:\n",
      "                if sampling_rate_check != current_fs1:\n",
      "                    raise Warning('Please make sure all files have the same sampling rate')\n",
      "        else:   # processing class B\n",
      "            [current_fs2, x2] = read(values['-PATH-'] + '/' + class_names[2] + '/' + filenames_2[i - len(filenames_1)])\n",
      "            x2 = x2[:, 0]  # grabbing only channel 1\n",
      "            time_series_b.extend(x2)\n",
      "            if sampling_rate_check != current_fs2:\n",
      "                raise Warning('Please make sure all files have the same sampling rate')\n",
      "    return chunk, hop, time_series_a, time_series_b, sampling_rate_check\n",
      "\n",
      "\n",
      "def getFeatures(time_series_a, time_series_b, chunk, hop, sampling_rate):\n",
      "    def helpGetFeatures(feature, file, f_s):\n",
      "        [vsf, t] = pyACA.computeFeature(feature, file, f_s, iBlockLength=chunk, iHopLength=hop)\n",
      "        return vsf\n",
      "    time_series_a = np.array(time_series_a)     # initializing variables\n",
      "    time_series_b = np.array(time_series_b)\n",
      "    features_a = []\n",
      "    features_b = []\n",
      "    for f in range(len(feature_names)):\n",
      "        if not sg.one_line_progress_meter('Feature Extraction Progress', f + 1, len(feature_names),\n",
      "                                          'step 2 of 3: feature extraction'):\n",
      "            break\n",
      "        current_feature_a = helpGetFeatures(feature_list[f], time_series_a, sampling_rate)\n",
      "        features_a.append(current_feature_a)\n",
      "        current_feature_b = helpGetFeatures(feature_list[f], time_series_b, sampling_rate)\n",
      "        features_b.append(current_feature_b)\n",
      "    return features_a, features_b\n",
      "\n",
      "\n",
      "def formatData():\n",
      "    data1 = np.ones(len(features_list_a[0]))\n",
      "    data2 = np.zeros(len(features_list_b[0]))\n",
      "    data3 = np.concatenate((data1, data2))\n",
      "    data5 = []\n",
      "    for d in range(len(feature_names)):\n",
      "        data4 = np.concatenate([features_list_a[d], features_list_b[d]])\n",
      "        data5.append(data4)\n",
      "    categories = pd.DataFrame(data3)\n",
      "    predictors = pd.DataFrame(data5).transpose()\n",
      "    predictors.columns = feature_names\n",
      "    predictors_scaled = predictors.copy()  # normalization of audio features\n",
      "    for i in feature_names:\n",
      "        predictors_scaled[i] = (predictors_scaled[i] - predictors_scaled[i].min()) / \\\n",
      "                               (predictors_scaled[i].max() - predictors_scaled[i].min())\n",
      "    training_categories, testing_categories, training_predictors, testing_predictors \\\n",
      "        = train_test_split(categories, predictors_scaled, test_size=.2, random_state=25)\n",
      "    return training_categories, testing_categories, training_predictors, testing_predictors\n",
      "\n",
      "\n",
      "def initializeModels():\n",
      "    model1 = LogisticRegression(solver='lbfgs', max_iter=200)\n",
      "    model2 = svm.SVC()\n",
      "    model3 = KNeighborsClassifier(n_neighbors=3)\n",
      "    model4 = RandomForestClassifier(max_depth=2, random_state=0)\n",
      "    return model1, model2, model3, model4\n",
      "\n",
      "[model1, model2, model3, model4] = initializeModels()\n",
      "\n",
      "\n",
      "optimalModel = model1\n",
      "\n",
      "\n",
      "[chunk, hop, time_series_a, time_series_b, sampling_rate_check] = readData('/Users/William/Desktop/AudioTech_2/id_00/', 5000, 2500)\n",
      "[features_a, features_b] = getFeatures(time_series_a, time_series_b, chunk, hop, sampling_rate_check)\n",
      "[training_categories, testing_categories, training_predictors, testing_predictors] = formatData(features_a, features_b)\n",
      "optimalModel.fit(training_predictors, training_categories)\n",
      "predictions = optimalModel.predict(testing_predictors)\n",
      "cm = confusion_matrix(testing_categories, predictions)\n",
      "pd.DataFrame(cm)\n",
      "accuracy = (cm[0,0]+cm[1,1]) / sum(sum(cm))\n"
     ]
    }
   ],
   "source": [
    "snippetGen('/Users/William/Desktop/AudioTech_2/id_00/',featList,optimalModel,5000,2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-8fff6024d7f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_series_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_series_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_rate_check\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreadData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/Users/William/Desktop/AudioTech_2/id_00/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfeatures_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_b\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_series_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_series_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_rate_check\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtraining_categories\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_categories\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_predictors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_predictors\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-111-8fff6024d7f0>\u001b[0m in \u001b[0;36mreadData\u001b[1;34m(path, chunk, hop)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mclass_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mfilenames_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mfilenames_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mnum_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilenames_1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilenames_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mtime_series_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# initializing variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from scipy.io.wavfile import read\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import pyACA\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import combinations\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def readData(path, chunk, hop):\n",
    "    #chunk = values['-CHUNK-']\n",
    "    #hop = (1 - (values['-HOP-']) / 100) * chunk\n",
    "    class_names = os.listdir(path)\n",
    "    filenames_1 = os.listdir(path + '/' + class_names[1])\n",
    "    filenames_2 = os.listdir(path + '/' + class_names[2])\n",
    "    num_files = len(filenames_1) + len(filenames_2)\n",
    "    time_series_a = []  # initializing variables\n",
    "    time_series_b = []\n",
    "    bool_check = True\n",
    "    for i in range(num_files):  # step 1 progress bar\n",
    "        if not sg.one_line_progress_meter('File Reading Progress', i + 1, num_files, 'step 1 of 3: file reading'):\n",
    "            break\n",
    "        if i < len(filenames_1):    # processing class A\n",
    "            [current_fs1, x] = read(path + '/' + class_names[1] + '/' + filenames_1[i])\n",
    "            x = x[:, 0]  # grabbing only channel 1 of recording\n",
    "            time_series_a.extend(x)\n",
    "            if bool_check:  # grabbing the first returned sampling-rate to check subsequent values against\n",
    "                sampling_rate_check = current_fs1\n",
    "                bool_check = False\n",
    "            else:\n",
    "                if sampling_rate_check != current_fs1:\n",
    "                    raise Warning('Please make sure all files have the same sampling rate')\n",
    "        else:   # processing class B\n",
    "            [current_fs2, x2] = read(values['-PATH-'] + '/' + class_names[2] + '/' + filenames_2[i - len(filenames_1)])\n",
    "            x2 = x2[:, 0]  # grabbing only channel 1\n",
    "            time_series_b.extend(x2)\n",
    "            if sampling_rate_check != current_fs2:\n",
    "                raise Warning('Please make sure all files have the same sampling rate')\n",
    "    return chunk, hop, time_series_a, time_series_b, sampling_rate_check\n",
    "\n",
    "\n",
    "def getFeatures(time_series_a, time_series_b, chunk, hop, sampling_rate):\n",
    "    def helpGetFeatures(feature, file, f_s):\n",
    "        [vsf, t] = pyACA.computeFeature(feature, file, f_s, iBlockLength=chunk, iHopLength=hop)\n",
    "        return vsf\n",
    "    time_series_a = np.array(time_series_a)     # initializing variables\n",
    "    time_series_b = np.array(time_series_b)\n",
    "    features_a = []\n",
    "    features_b = []\n",
    "    for f in range(len(feature_names)):\n",
    "        if not sg.one_line_progress_meter('Feature Extraction Progress', f + 1, len(feature_names),\n",
    "                                          'step 2 of 3: feature extraction'):\n",
    "            break\n",
    "        current_feature_a = helpGetFeatures(feature_list[f], time_series_a, sampling_rate)\n",
    "        features_a.append(current_feature_a)\n",
    "        current_feature_b = helpGetFeatures(feature_list[f], time_series_b, sampling_rate)\n",
    "        features_b.append(current_feature_b)\n",
    "    return features_a, features_b\n",
    "\n",
    "\n",
    "def formatData():\n",
    "    data1 = np.ones(len(features_list_a[0]))\n",
    "    data2 = np.zeros(len(features_list_b[0]))\n",
    "    data3 = np.concatenate((data1, data2))\n",
    "    data5 = []\n",
    "    for d in range(len(feature_names)):\n",
    "        data4 = np.concatenate([features_list_a[d], features_list_b[d]])\n",
    "        data5.append(data4)\n",
    "    categories = pd.DataFrame(data3)\n",
    "    predictors = pd.DataFrame(data5).transpose()\n",
    "    predictors.columns = feature_names\n",
    "    predictors_scaled = predictors.copy()  # normalization of audio features\n",
    "    for i in feature_names:\n",
    "        predictors_scaled[i] = (predictors_scaled[i] - predictors_scaled[i].min()) / \\\n",
    "                               (predictors_scaled[i].max() - predictors_scaled[i].min())\n",
    "    training_categories, testing_categories, training_predictors, testing_predictors \\\n",
    "        = train_test_split(categories, predictors_scaled, test_size=.2, random_state=25)\n",
    "    return training_categories, testing_categories, training_predictors, testing_predictors\n",
    "\n",
    "\n",
    "def initializeModels():\n",
    "    model1 = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "    model2 = svm.SVC()\n",
    "    model3 = KNeighborsClassifier(n_neighbors=3)\n",
    "    model4 = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    return model1, model2, model3, model4\n",
    "\n",
    "[model1, model2, model3, model4] = initializeModels()\n",
    "\n",
    "\n",
    "optimalModel = model1\n",
    "\n",
    "\n",
    "[chunk, hop, time_series_a, time_series_b, sampling_rate_check] = readData('/Users/William/Desktop/AudioTech_2/id_00/', 5000, 2500)\n",
    "[features_a, features_b] = getFeatures(time_series_a, time_series_b, chunk, hop, sampling_rate_check)\n",
    "[training_categories, testing_categories, training_predictors, testing_predictors] = formatData(features_a, features_b)\n",
    "optimalModel.fit(training_predictors, training_categories)\n",
    "predictions = optimalModel.predict(testing_predictors)\n",
    "cm = confusion_matrix(testing_categories, predictions)\n",
    "pd.DataFrame(cm)\n",
    "accuracy = (cm[0,0]+cm[1,1]) / sum(sum(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
